# ğŸ“„ PDF Summarizer + Chat (RAG)

## ğŸ” About
A **lightweight PDF (upto 500+ pages) summarizer and chat app powered by local LLMs**.  
Upload a PDF, get instant summaries, and ask questions about the content â€” perfect for cutting through lengthy docs, research papers, or legal files.

The app runs **fully locally** using [Ollama](https://ollama.ai) for inference, FAISS for vector search, and FastAPI as the backend API server.  
This ensures **data privacy, low latency, and zero external dependencies**.

---

## ğŸš€ Features
- ğŸ“„ **Upload PDFs** and instantly parse them.
- âœ‚ï¸ **Chunking + FAISS embeddings** for efficient semantic retrieval.
- ğŸ“ **Automatic summarization** of uploaded documents.
- ğŸ’¬ **Chat with the PDF** using a Retrieval-Augmented Generation (RAG) pipeline.
- â³ **Progress box** with animated progress bar while upload + processing is in progress.
- ğŸ“œ **History panel** of summaries and chat interactions.
- âš¡ Runs **entirely on local hardware** with no cloud dependency.

---

## ğŸ–¥ï¸ Frontend

### Tech Stack
- **React** â€“ UI framework
- **Redux** â€“ state management
- **CSS / Tailwind** â€“ styling
- **React Draggable** â€“ draggable progress/status components

### Components
- **App.jsx** â€“ Root container orchestrating UI.
- **Chat.jsx** â€“ Chat input and output view.
- **History.jsx** â€“ Scrollable history of interactions.
- **UploadProgressBox.jsx** â€“ Small draggable upload/processing indicator.

### Flow
1. User uploads a PDF â†’ triggers `/upload`.
2. Progress box appears with striped animated loader.
3. Frontend polls `/status` until processing = `ready`.
4. User can click **Summarize** or chat with the PDF.
5. Summaries + conversations are added to history.

---

## âš™ï¸ Backend

### Tech Stack
- **FastAPI** â€“ REST API backend
- **Ollama** â€“ Local LLM inference engine
- **SentenceTransformers** â€“ Embedding model
- **FAISS** â€“ Vector store for retrieval
- **PyPDF2 / pdfplumber** â€“ PDF parsing
- **Uvicorn** â€“ ASGI server

### Core Files
- `main.py` â€“ FastAPI endpoints (`/upload`, `/status`, `/summarize`, `/chat`).
- `embedder.py` â€“ Builds embeddings + FAISS index.
- `rag.py` â€“ RAG pipeline integrating Ollama + FAISS.
- `utils.py` â€“ PDF text extraction, chunking helpers.

### Endpoints
- **`POST /upload`**  
  - Accepts PDF.  
  - Extracts + chunks text.  
  - Embeds chunks into FAISS.  
  - Marks status â†’ `processing`.

- **`GET /status`**  
  - Returns whether the backend is `ready` for summarization/chat.

- **`POST /summarize`**  
  - Uses Ollama to summarize the entire document.

- **`POST /chat`**  
  - Accepts user query.  
  - Retrieves top-k chunks from FAISS.  
  - Combines with query â†’ sends to Ollama.  
  - Returns contextual answer.

---

## ğŸ”„ Workflow
1. Upload PDF â†’ `/upload` â†’ progress box shown.
2. Backend processes â†’ embeddings + FAISS index created.
3. Frontend polls `/status` until `ready`.
4. User can:
   - Generate summary (`/summarize`).
   - Ask contextual questions (`/chat`).
5. History view updated in real-time.

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Backend (FastAPI)
```bash
cd backend
python -m venv venv
source venv/bin/activate   # (Mac/Linux)
venv\Scripts\activate      # (Windows)

pip install -r requirements.txt
uvicorn main:app --reload
```
âš ï¸ Requires Ollama installed and running locally:
```bash ollama run llama2
```

Backend will run at:
ğŸ‘‰ ```http://127.0.0.1:8000```

### 
```bash
cd frontend
npm install
npm run dev
```

Frontend will run at:
ğŸ‘‰ ```http://localhost:5173/```

---

## ğŸ”— API Flow

1. Upload PDF â†’ POST /upload_pdf/
- Returns session_id.

2. Check status â†’ GET /status/{session_id}
- Keep polling until "ready".
  
3. Summarize â†’ POST /summarize/{session_id}
- Returns summarized text.

4. Chat â†’ POST /chat/{session_id}
- Input: Question, Output: Answer.

5. History â†’ GET /history/
- Returns all previous sessions.
